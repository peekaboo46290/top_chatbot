from langchain_ollama import OllamaEmbeddings
from langchain_ollama import ChatOllama
from langchain_neo4j import Neo4jVector
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    SystemMessagePromptTemplate,
)
from base_logger import logger




def load_embedding_model(logger=logger, config={}):
    embedding = OllamaEmbeddings(
        base_url=config["ollama_base_url"], model=config["llm"]
    )
    dimension = 4096
    logger.info("Embedding: Using Ollama")
    return embedding, dimension 


def load_llm(llm_name:str, ollama_base_url:str, logger = logger):
    try:
        llm =  ChatOllama(
            temperature=0,
            base_url=ollama_base_url,
            model=llm_name,
            streaming=True,
            # seed=2,
            top_k=10,  # A higher value (100) will give more diverse answers, while a lower value (10) will be more conservative.
            top_p=0.3,  # Higher value (0.95) will lead to more diverse text, while a lower value (0.5) will generate more focused text.
            num_ctx=3072,  # Sets the size of the context window used to generate the next token.
        )
        logger.info(f"Loaded llm: {llm_name}")
        return llm
    except Exception as e:
        logger.info(f"failed to load llm. error: {e}")

def configure_llm_only_chain(llm):
    template = """
    You are a helpful assistant that helps a support agent with answering math questions.
    If you don't know the answer, just say that you don't know, you must not make up an answer.
    """
    system_message_prompt = SystemMessagePromptTemplate.from_template(template)
    human_template = "{question}"
    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
    chat_prompt = ChatPromptTemplate.from_messages(
        [system_message_prompt, human_message_prompt]
    )
    chain = chat_prompt | llm | StrOutputParser()
    return chain

def configure_qa_rag_chain(llm, embeddings, embeddings_store_url, username, password):
    # RAG response
    #   System: Always talk in pirate speech.
    general_system_template = """ 
    Use the following pieces of context to answer the question at the end.
    The context contains theorem and definition and some have example added to them with u.
    Make sure to rely on information from the answers and not on questions to provide accurate responses.
    When you find particular answer in the context useful, make sure to cite it in the answer using 
    the name of theorem if provided else use and what chapter it is under.
    If you don't know the answer, just say that you don't know, don't try to make up an answer.
    ----
    {summaries}
    ----
    Each answer you generate should contain a section at the end of what chapter it is from.
    You can only use theorem that are present in the context and always
    add what chapter it is from to the end of the answer in the style of citations.
    Generate concise answers with references sources section of what chapter it is from to 
    relevant theorem only at the end of the answer.
    """
    general_user_template = "Question:```{question}```"
    messages = [
        SystemMessagePromptTemplate.from_template(general_system_template),
        HumanMessagePromptTemplate.from_template(general_user_template),
    ]
    qa_prompt = ChatPromptTemplate.from_messages(messages)

    # Vector + Knowledge Graph response
    kg = Neo4jVector.from_existing_index(
        embedding=embeddings,
        url=embeddings_store_url,
        username=username,
        password=password,
        database="neo4j",  # neo4j by default
        index_name="theorem",  # vector by default
        text_node_property="body",  # text by default
        retrieval_query="""
    WITH node AS question, score AS similarity
    CALL  { with question
        MATCH (question)<-[:ANSWERS]-(answer)
        WITH answer
        ORDER BY answer.is_accepted DESC, answer.score DESC
        WITH collect(answer)[..2] as answers
        RETURN reduce(str='', answer IN answers | str + 
                '\n### Answer (Accepted: '+ answer.is_accepted +
                ' Score: ' + answer.score+ '): '+  answer.body + '\n') as answerTexts
    } 
    RETURN '##Question: ' + question.title + '\n' + question.body + '\n' 
        + answerTexts AS text, similarity as score, {source: question.link} AS metadata
    ORDER BY similarity ASC // so that best answers are the last
    """,
    )
    
    # kg_qa = (
    #     RunnableParallel(
    #         {
    #             "summaries": kg.as_retriever(search_kwargs={"k": 2}) | format_docs,
    #             "question": RunnablePassthrough(),
    #         }
    #     )
    #     | qa_prompt
    #     | llm
    #     | StrOutputParser()
    # )
    # return kg_qa
